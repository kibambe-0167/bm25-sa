{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(text: list):\n",
    "  \"\"\"text: list of text documents/sentences\"\"\"\n",
    "  text = ' '.join(str(text).lower())\n",
    "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun set over the horizon, casting a warm glow across the fields.',\n",
       " 'She carefully placed the delicate vase on the table, admiring its intricate design.',\n",
       " 'The sound of the rain tapping on the window created a soothing atmosphere.',\n",
       " 'He opened the old book and was immediately transported to a different time and place.',\n",
       " 'The children laughed and played in the park, their joy infectious to everyone around.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"The sun set over the horizon, casting a warm glow across the fields.\",\n",
    "    \"She carefully placed the delicate vase on the table, admiring its intricate design.\",\n",
    "    \"The sound of the rain tapping on the window created a soothing atmosphere.\",\n",
    "    \"He opened the old book and was immediately transported to a different time and place.\",\n",
    "    \"The children laughed and played in the park, their joy infectious to everyone around.\",\n",
    "]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 15, 14, 15)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi = min([len(i.split(' ')) for i in data])\n",
    "mx = max([len(i.split(' ')) for i in data])\n",
    "avg = round(sum( [len(d.split(' ')) for d in data] ) / len(data))\n",
    "avg_max = math.ceil((avg + mx) / 2)\n",
    "mi, mx, avg, avg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 13, 13, 15, 14]\n",
      "tl 14\n",
      "l 13\n",
      "l 13\n",
      "l 13\n",
      "g 14\n",
      "g 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 14, 14, 14, 14]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([ len(d.split()) for d in data])\n",
    "tl = []\n",
    "tlen = avg\n",
    "print(\"tl\", tlen)\n",
    "sp = \"aaaa\"\n",
    "for d in data:\n",
    "  d = ' '.join(d.split()).lower().split(' ')\n",
    "  if len(d) < tlen:\n",
    "    print(\"l\", len(d),  )\n",
    "    d.extend( [ sp for _ in range(len(d), tlen)])\n",
    "  else:\n",
    "    print(\"g\", len(d[: tlen ]))\n",
    "    d = d[: tlen]\n",
    "  tl.append(d)\n",
    "\n",
    "# print(tl)\n",
    "[ len(d) for d in tl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1920929e-07"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25():\n",
    "  \"\"\"\n",
    "  data: list of strings\n",
    "  token_len: avg | min | max | avg_max[avg of avg and max]\n",
    "  freq_sat - frequency_saturation : float\n",
    "  len-norm - length of normalisation : float\n",
    "  token_len - tokens length per document ot have same length in all docs\n",
    "  \"\"\"\n",
    "  def __init__(self, data: list, freq_sat=0.2, len_norm=2, tokens_len='avg'):\n",
    "    self.data = data\n",
    "    self.freq_sat = freq_sat\n",
    "    self.len_norm = len_norm\n",
    "    self.tokens_len = tokens_len.lower()\n",
    "    # used for filling short documents to make documents have equal lengths\n",
    "    self.special_token: float = np.finfo(np.float32).eps # sys.float_info.epsilon # use the\n",
    "    self.normalise_data()\n",
    "  \n",
    "  def normalise_data(self): self.data = [ str(doc).lower() for doc in self.data ]\n",
    "\n",
    "  def TF(self, t, d):\n",
    "    f = self.freq_term_document(t, d)\n",
    "    n = f * self.freq_saturation()\n",
    "    dd = f + (self.freq_sat * ( 1 - self.len_norm + self.len_norm * ( self.len_doc(d) / self.avg_doc_length() )))\n",
    "    return n / dd\n",
    "  \n",
    "  def freq_saturation(self,): return self.freq_sat + 1\n",
    "  \n",
    "  def len_doc(self, d): return len( d.split(\" \") )\n",
    "  \n",
    "  def avg_doc_length(self,): \n",
    "    return sum( [ len(d.split(\" \")) for d in self.data ] ) / self.__len__()\n",
    "  \n",
    "  def freq_term_document(self, t, d): return d.split(\" \").count(t)\n",
    "  \n",
    "  def IDF(self, t):\n",
    "    n = self.__len__() - self.numb_doc_with_term(t) + .5\n",
    "    d = self.numb_doc_with_term(t) + .5\n",
    "    y = np.log((n/d)+1)\n",
    "    return y\n",
    "\n",
    "  def transform(self):\n",
    "    \"\"\" variant, modified to fit my use case \"\"\"\n",
    "    res = []\n",
    "    for doc in self.data: \n",
    "      x = [ self.TF(t, doc) * self.IDF(t) for t in doc.split(\" \") ]\n",
    "      res.append(x)\n",
    "    res = self._token_length(res)\n",
    "    return res\n",
    "\n",
    "  def numb_doc_with_term(self, t):\n",
    "    c = 0;\n",
    "    for doc in self.data: \n",
    "      if t in doc: c += 1\n",
    "    return c\n",
    "  \n",
    "  def len_document(self, d): \n",
    "    return len(d.split(\" \"))\n",
    "  \n",
    "  def __len__(self): return len(self.data)\n",
    "  \n",
    "  def _token_len_min(self):\n",
    "    return min([len(i.split(' ')) for i in self.data])\n",
    "  \n",
    "  def _token_len_max(self):\n",
    "    return max([len(i.split(' ')) for i in self.data])\n",
    "  \n",
    "  def _token_len_avg(self):\n",
    "    return round(sum( [len(d.split(' ')) for d in self.data] ) / len(self.data))\n",
    "  \n",
    "  def _vocabulary(self):\n",
    "    '''\n",
    "    - store unique words from the entire document\n",
    "    - store vocab using their bm25, so that its can be easily to map back from bm25 number to word used\n",
    "    '''\n",
    "    pass\n",
    "  \n",
    "  def _vocabulary_length(self):\n",
    "    '''get the length of unique words from the data corpus'''\n",
    "    x = ' '.join(self.data)\n",
    "    x = ' '.join(x.split()).lower().split(' ')\n",
    "    return len(set(x))\n",
    "    \n",
    "  def _token_length(self, embs ):\n",
    "    tl = []\n",
    "    tlen = 0\n",
    "    # \n",
    "    if self.tokens_len == 'min': tlen = self._token_len_min()\n",
    "    elif self.tokens_len == 'max': tlen = self._token_len_max()\n",
    "    elif self.tokens_len == 'avg_max': tlen = math.ceil( (self._token_len_max() + self._token_len_avg() ) / 2 )\n",
    "    else : tlen = self._token_len_avg()\n",
    "    # \n",
    "    for d in embs:\n",
    "      if len(d) < tlen: d.extend( [ self.special_token for _ in range(len(d), tlen)])\n",
    "      else: d = d[: tlen]\n",
    "      tl.append(d)\n",
    "      \n",
    "    return tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 14, 14, 14, 14]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm23 = BM25(data)\n",
    "x = bm23.transform()\n",
    "\n",
    "# for i in x: print(i)\n",
    "[len(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-bm25-sa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
